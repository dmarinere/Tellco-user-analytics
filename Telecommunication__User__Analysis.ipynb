{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing our Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting our columns to display full values\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', '{:20,.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xlrd in /home/mahveotm/.local/lib/python3.8/site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "#i had to install this to work on this excel file \n",
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading our Dataset and Column Descriptions to our notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the data\n",
    "df = pd.read_excel(\"Week2_challenge_data_source.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File Field Descriptions - Sheet1.csv does not exist: 'Field Descriptions - Sheet1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b3d4896316c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#importing the data description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_descript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Field Descriptions - Sheet1.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File Field Descriptions - Sheet1.csv does not exist: 'Field Descriptions - Sheet1.csv'"
     ]
    }
   ],
   "source": [
    "#importing the data description\n",
    "df_descript = pd.read_csv(\"Field Descriptions - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all columns with space and lowering the  letters of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [label.replace(' ', '_').lower() for label in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring our Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to try to understand the data here and check for missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with understanding what different columns stand for, here is a view of the different columns meaning this helps others understand this data too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_descript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us the different data types and number of non-null values in each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us the Numbers of Rows and Columns in our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers of unique values in each columns are shown below to understand a little bit, how the data is distributed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Null Values in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to analyze my dataset to understand where the null values are and how to deal with them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i am trying to understanding the null values here by analyzing the percentage of missing values in each columns\n",
    "null = df.isna().sum().sort_values(ascending = False)\n",
    "null_per = (null/150001) * 100\n",
    "null_perc = pd.DataFrame(null_per)\n",
    "null_perc.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have noticed the columns with a lot of missing values and would decide now how to impute this, i would be mean as already stated for numerical columns and median to replace null for Categorical columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would start with getting all the missing numerical values in our missing data, i would turn all the values in null percent to a list of columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_perc.reset_index(inplace=True)\n",
    "missing = list(null_perc['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would append numerical values that are missing to one list and the categorical columns to another list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numcols =[]\n",
    "catcols = []\n",
    "for data in missing:\n",
    "    if df[data].dtypes in numerics:\n",
    "        numcols.append(data)\n",
    "    else:\n",
    "        catcols.append(data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some values that are numerical but are not supposed to be like the IMEI, IMSI,MSISDN we would remove them from our list of numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we would create a new list for the categorical values remove them from the numerical list and add to the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catcoladd =['imei', 'imsi','msisdn/number']\n",
    "for c in catcoladd:\n",
    "    numcols.remove(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catcols.extend(catcoladd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would start with filling the missing values in the numerical column with the mean of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numcols:\n",
    "    df[col] = df[col].fillna(df[col].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm if what we did work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numcols].isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we would try to fill the categorical variables by the mode since we cannot use mean here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we would have to convert all the new columns we added to object data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[catcoladd] = df[catcoladd].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in catcols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we would check all our categorical column to see if what we did worked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[catcols].isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Overview Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying the top 10 handsets used by the customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten = df['handset_type'].value_counts(ascending = False).head(10).reset_index()\n",
    "top_ten = top_ten.rename(columns={'index':\"Handset Type\",\"handset_type\":\"Count\"})\n",
    "top_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(x = top_ten['Handset Type'], y = top_ten['Count'])\n",
    "plt.xlabel('Name of handsets')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('A barplot showing the top ten handset types');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 3 Handset Manufacturer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_three = df['handset_manufacturer'].value_counts(ascending = False).head(3).reset_index()\n",
    "top_three = top_three.rename(columns={'index':\"Handset Manufacturer\",\"handset_manufacturer\":\"Count\"})\n",
    "top_three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 handset per Handset Manufacturer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_fivemanf = df.groupby(['handset_manufacturer','handset_type'])['handset_type'].count().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_fivemanf=top_fivemanf.sort_values(['handset_manufacturer', 'count']).groupby('handset_manufacturer').head(5)\n",
    "top_fivemanf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Make a short interpretation to the marketing team \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Top ten handset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    From the list of top ten Handset we can observe that the most purchase unique Handset is *Huawei B528S-23A* is our most used handset while Apple IPhone is the most purchased in our Top ten Handset, Samsung also appear in the list as one of the top ten phone\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Recommendations\n",
    "        we should try to make our service tailored for this brands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of Users Behavior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bearer ID according to our data description is unique for each session, the bearer ID is our unique session identifier so to identify each session we would the bearer ID here to group them, and count them to know the total number of Unique sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis we would be using MSISDN as a unique identifier for each users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  number of xDR sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bearer_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    There is a misnomer in the first row in our Bearer ID\n",
    "    I dropped all rows without Bearer ID, since we\n",
    "    cannot confirm if they had more than one session or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the total number of unique Session in our dataset is the number of unique Bearer ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bearer_id = pd.DataFrame(df[['msisdn/number','bearer_id']])\n",
    "bearer_id= bearer_id.loc[bearer_id['bearer_id'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumberOfSessions=bearer_id.groupby('msisdn/number').agg({'bearer_id': 'count'}).reset_index()\n",
    "NumberOfSessions=NumberOfSessions.rename(columns= {'bearer_id': 'xDR sessions'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NumberOfSessions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 6))\n",
    "sns.distplot(NumberOfSessions[\"xDR sessions\"], hist=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SessionDuration =df.groupby('msisdn/number').agg({'dur._(ms).1': 'sum'}).reset_index()\n",
    "#here is the duration for first five rows in our dataset\n",
    "SessionDuration=SessionDuration.rename(columns= {'dur._(ms).1': 'sessions duration(ms)'})\n",
    "SessionDuration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SessionDuration['dur._(ms).1'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(SessionDuration[\"dur._(ms).1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(SessionDuration[\"dur._(ms).1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SessionDuration.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UpDownload_user = df[['msisdn/number','total_ul_(bytes)','total_dl_(bytes)']]\n",
    "UpDownload_user = UpDownload_user.groupby('msisdn/number').sum().reset_index()\n",
    "UpDownload_user.drop(['msisdn/number'], axis=1, inplace=True)\n",
    "UpDownload_user.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(30, 10))\n",
    "ax[0].hist(total_updown);\n",
    "sns.boxplot(UpDownload_user[\"Applications UL/DL (bytes)\"],ax=ax[1]);\n",
    "ax[0].title.set_text('Histogram showing the Total  Data Usage by all Applications in Bytes')\n",
    "ax[1].title.set_text('Box Plot showing the Total Data Usage by all Applications in Bytes')\n",
    "ax[2].title.set_text('Distribution Plot showing the Total Data Usage  by all Applications in Bytes')\n",
    "sns.distplot(total_updown[\"Applications UL/DL (bytes)\"], ax=ax[2]);\n",
    "fig.savefig('Applications.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicationDownUp =df[['social_media_dl_(bytes)','social_media_ul_(bytes)', 'google_dl_(bytes)',\n",
    "                        'google_ul_(bytes)', 'email_dl_(bytes)', 'email_ul_(bytes)', 'youtube_dl_(bytes)',\n",
    "                        'youtube_ul_(bytes)','netflix_dl_(bytes)', 'netflix_ul_(bytes)', 'gaming_dl_(bytes)',\n",
    "                        'gaming_ul_(bytes)','other_dl_(bytes)', 'other_ul_(bytes)']]\n",
    "total_updown = pd.DataFrame()\n",
    "total_updown[\"social_media_total(bytes)\"] = df[\"social_media_dl_(bytes)\"] + df['social_media_ul_(bytes)']\n",
    "total_updown[\"google_total(bytes)\"] = df[\"google_dl_(bytes)\"] + df[\"google_ul_(bytes)\"]\n",
    "total_updown['email_total(bytes)'] = df[\"email_dl_(bytes)\"] + df[\"email_ul_(bytes)\"]\n",
    "total_updown['youtube_total(bytes)'] = df[\"youtube_dl_(bytes)\"] + df[\"youtube_ul_(bytes)\"]\n",
    "total_updown['netflix_total(bytes)'] = df[\"netflix_dl_(bytes)\"] + df[\"netflix_ul_(bytes)\"]\n",
    "total_updown[\"gaming_total(bytes)\"] = df[\"gaming_dl_(bytes)\"] + df[\"gaming_ul_(bytes)\"]\n",
    "total_updown['other_total(bytes)']=df[\"other_dl_(bytes)\"]+df[\"other_ul_(bytes)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_updown[\"Total Applications (bytes)\"]= total_updown['social_media_total(bytes)'] + total_updown['google_total(bytes)']+ total_updown['email_total(bytes)']+ total_updown['youtube_total(bytes)']+ total_updown['netflix_total(bytes)']+ total_updown['gaming_total(bytes)']+ total_updown['other_total(bytes)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_updown.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.DataFrame(total_updown.sum(axis=0, skipna=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new =new.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new= new.rename(columns={'index':\"Applications\",0:\"Sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = new[:7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "ax = sns.barplot(x=\"Applications\", y=\"Sum\", data=new)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"A Bar plot showing the data consumption for each Applications\",fontsize = 12, fontweight= \"bold\")\n",
    "plt.xticks(fontsize = 10, fontweight= \"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"totaL.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_updown[\"Total Applications (bytes)\"] = total_updown[\"Total Applications (bytes)\"].astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(30, 10))\n",
    "ax[0].hist(total_updown);\n",
    "sns.boxplot(total_updown[\"Total Applications (bytes)\"],ax=ax[1]);\n",
    "ax[0].title.set_text('Histogram showing the Total  Data Usage by all Applications in Bytes')\n",
    "ax[1].title.set_text('Box Plot showing the Total Data Usage by all Applications in Bytes')\n",
    "ax[2].title.set_text('Distribution Plot showing the Total Data Usage  by all Applications in Bytes')\n",
    "sns.distplot(total_updown[\"Total Applications (bytes)\"], ax=ax[2]);\n",
    "fig.savefig('Applications.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(30, 10))\n",
    "ax[0].hist(total_updown[\"Total Applications (bytes)\"]);\n",
    "sns.boxplot(total_updown[\"Total Applications (bytes)\"], ax=ax[1]);\n",
    "ax[0].title.set_text('Histogram showing the Total  Data Usage by all Applications in Bytes')\n",
    "ax[1].title.set_text('Box Plot showing the Total Data Usage by all Applications in Bytes')\n",
    "ax[2].title.set_text('Distribution Plot showing the Total Data Usage  by all Applications in Bytes')\n",
    "sns.distplot(total_updown[\"Total Applications (bytes)\"], ax=ax[2]);\n",
    "fig.savefig('Applications.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kurtosis for Duration of Call is\", pd.DataFrame(total_updown[\"email_total(bytes)\"]).kurtosis()[0], \"Skewedness of Duration of call is\",\n",
    "      pd.DataFrame(total_updown[\"email_total(bytes)\"]).skew()[0],\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(30, 10))\n",
    "ax[0].hist(total_updown[\"youtube_total(bytes)\"]);\n",
    "sns.boxplot(total_updown[\"youtube_total(bytes)\"], ax=ax[1]);\n",
    "ax[0].title.set_text('Histogram showing the Total  Data Usage on Youtube in Bytes')\n",
    "ax[1].title.set_text('Box Plot showing the Total Data Usage on Youtube in Bytes')\n",
    "ax[2].title.set_text('Distribution Plot showing the Total Data Usage on Youtube in Bytes')\n",
    "sns.distplot(total_updown[\"youtube_total(bytes)\"], ax=ax[2]);\n",
    "fig.savefig('Youtube.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kurtosis for the total Upload/Download is\", pd.DataFrame(total_updown[\"youtube_total(bytes)\"]).kurtosis()[0], \"Skewedness of Duration of call is\",\n",
    "      pd.DataFrame(total_updown[\"youtube_total(bytes)\"]).skew()[0],\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(30, 10))\n",
    "ax[0].hist(total_updown[\"netflix_total(bytes)\"]);\n",
    "sns.boxplot(total_updown[\"netflix_total(bytes)\"], ax=ax[1]);\n",
    "ax[0].title.set_text('Histogram showing the Total Netflix Data Usage in Bytes')\n",
    "ax[1].title.set_text('Box Plot showing the Total Netflix Data Usage in Bytes')\n",
    "ax[2].title.set_text('Distribution Plot showing the Total Netflix Data Usage in Bytes')\n",
    "sns.distplot(total_updown[\"netflix_total(bytes)\"], ax=ax[2]);\n",
    "fig.savefig('netflix.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bita29ee28c41394fccafaa8b30a5cd782b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
